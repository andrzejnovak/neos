---

title: neos.fit

keywords: fastai
sidebar: home_sidebar

summary: "Module containing functions to perform maximum likelihod fits in a differentiable way."
description: "Module containing functions to perform maximum likelihod fits in a differentiable way."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/03_fit.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>fax</code> module is leveraged to calculate gradients of maximum likelihood fits through the 'two-phase' method, which is a technique for automatic differentiation of functions with an attractive fixed point, i.e. they satisfy $f(x) - x = 0$. This is the case for a minimization routine, where the minimization occurs from the minimum: <code>minimize(f, x_init) = x_min</code> --&gt; <code>minimize(f, x_init=x_min) = x_min</code>. You can imagine that the initial iterations won't give any useful information in terms of the gradient of the minimum; this approach leverages this fact by performing the minimization a second time in the neighborhood of the fixed point, and keeping track of gradients there, which avoids the unnecessary unrolling of many loops for the early iteration.</p>
<p>To read more about this method, see section 2.3 of <a href="https://www.researchgate.net/profile/Ala_Taftaf2/publication/323176030_ADJOINTS_OF_FIXED-POINT_ITERATIONS/links/5a8465644585159152b7fe00/ADJOINTS-OF-FIXED-POINT-ITERATIONS.pdf">this paper</a> that looks at some methods to approach the fixed-point differation problem within automatic differentiation.</p>
<p>The fits themselves are done by gradient descent with Adam.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_solvers" class="doc_header"><code>get_solvers</code><a href="https://github.com/phinate/neos/tree/master/neos/fit.py#L14" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_solvers</code>(<strong><code>model_constructor</code></strong>, <strong><code>pdf_transform</code></strong>=<em><code>False</code></em>, <strong><code>default_rtol</code></strong>=<em><code>1e-10</code></em>, <strong><code>default_atol</code></strong>=<em><code>1e-10</code></em>, <strong><code>default_max_iter</code></strong>=<em><code>10000000</code></em>, <strong><code>learning_rate</code></strong>=<em><code>0.01</code></em>)</p>
</blockquote>
<p>Wraps a series of functions that perform maximum likelihood fitting in the
<code>two_phase_solver</code> method found in the <code>fax</code> python module. This allows for
the calculation of gradients of the best-fit parameters with respect to upstream
parameters that control the underlying model, i.e. the event yields (which are
then parameterized by weights or similar).</p>
<p>Args:
        model_constructor: Function that takes in the parameters of the observable,
        and returns a model object (and background-only parameters)
Returns:
        g_fitter, c_fitter: Callable functions that perform global and constrained fits
        respectively. Differentiable :)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

