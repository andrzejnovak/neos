# AUTOGENERATED! DO NOT EDIT! File to edit: 00_nn.ipynb (unless otherwise specified).

__all__ = ['one_hot', 'simple_classifier']

# Cell
import jax.numpy as np
from jax.experimental import stax
from jax.experimental.stax import Dense, Relu, LogSoftmax
from jax import jit, grad, random
from jax.random import multivariate_normal, PRNGKey
from jax.config import config
from jax.experimental import optimizers
import numpy.random as npr
import itertools
import time
import matplotlib.pyplot as plt

# Cell
def one_hot(x, k, dtype=np.float32):
    """Create a one-hot encoding of x of size k. Taken from jax/examples."""
    return np.array(x[:, None] == np.arange(k), dtype)

# Cell
class simple_classifier:
    '''A simple classifier trained on separating two close normal distributions. Modelled after jax/examples/mnist_classifier.py.'''
    def __init__(self,rng=PRNGKey(0)):
        self.init_random_params, self.predict = stax.serial(
        Dense(10), Relu,
        Dense(2), LogSoftmax)

        self.jrng = rng

        sig = multivariate_normal(PRNGKey(1), np.asarray([2,5]), np.asarray([[1,0],[0,1]]),shape=(1,10000))[0]
        bkg = multivariate_normal(PRNGKey(1), np.asarray([3,6]), np.asarray([[1,0],[0,1]]),shape=(1,10000))[0]
        sig_lables = np.ones((1,10000))[0]
        bkg_lables = np.zeros((1,10000))[0]

        self.train_data = np.concatenate((sig,bkg))
        self.train_labels = one_hot(np.concatenate((sig_lables,bkg_lables)),2)

        self.params = []

    def __call__(self,x):
        return self.predict(self.params,x)

    def plot_train_data(self):
        plt.scatter(self.train_data[:,0], self.train_data[:,1],c=self.train_labels[:,0],alpha=0.2,s=1.4)
        plt.show()

    def train(self,step_size = 0.001,num_epochs = 3,batch_size = 16,momentum_mass = 0.9):
        num_train = self.train_data.shape[0]
        num_complete_batches, leftover = divmod(num_train, batch_size)
        num_batches = num_complete_batches + bool(leftover)

        def data_stream():
            rng = npr.RandomState(0)
            while True:
                perm = rng.permutation(num_train)
                for i in range(num_batches):
                    batch_idx = perm[i * batch_size:(i + 1) * batch_size]
                    yield self.train_data[batch_idx], self.train_labels[batch_idx]
        batches = data_stream()

        opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)

        def loss(params, batch):
            inputs, targets = batch
            preds = self.predict(params, inputs)
            return -np.mean(np.sum(preds * targets, axis=1))

        def accuracy(params, batch):
            inputs, targets = batch
            target_class = np.argmax(targets, axis=1)
            predicted_class = np.argmax(self.predict(params, inputs), axis=1)
            return np.mean(predicted_class == target_class)

        @jit
        def update(i, opt_state, batch):
            params = get_params(opt_state)
            return opt_update(i, grad(loss)(params, batch), opt_state)

        _, init_params = self.init_random_params(self.jrng, (-1, 2))
        if self.params == []:
            opt_state = opt_init(init_params)
        else:
            opt_state = opt_init(self.params)
        itercount = itertools.count()

        print("\nStarting training...")
        for epoch in range(num_epochs):
            start_time = time.time()
            for _ in range(num_batches):
                opt_state = update(next(itercount), opt_state, next(batches))
                #print("Loss {}".format(loss(params,next(batches))))
            epoch_time = time.time() - start_time

            params = get_params(opt_state)
            train_acc = accuracy(params, (self.train_data, self.train_labels))
            #test_acc = accuracy(self.params, (test_data, test_labels))
            print("Epoch {} in {:0.2f} sec".format(epoch, epoch_time))
            print("Training set accuracy {}".format(train_acc))
            #print("Test set accuracy {}".format(test_acc))

        self.params = params
    def plot_decision(self,plot_data=False):
        delta = 0.05
        x = np.arange(-2.0, 8.0, delta)
        y = np.arange(0, 11.0, delta)
        X, Y = np.meshgrid(x, y)
        pairs = np.dstack([X, Y]).reshape(-1, 2)
        Z = np.exp(self.predict(self.params,pairs))[:,0].reshape(X.shape)
        fig, ax = plt.subplots()
        CS = ax.contourf(X, Y, Z,cmap='magma')
        ax.set_title('Decision contours of simple_classifier')
        if plot_data:
            ax.scatter(self.train_data[:,0], self.train_data[:,1],c=self.train_labels[:,1],alpha=0.2,s=1.4)
        plt.show()
