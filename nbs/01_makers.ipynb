{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp makers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neos.makers\n",
    "\n",
    "> Functions that define the workflow from parametric observable --> statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module contains example implementations of functions that are composed such that everything downstream is a function of the parameters of your observable.\n",
    "\n",
    "\n",
    "- `hists_from_nn_three_blobs(predict)` uses the nn decision function `predict` to form histograms from signal and background data, all drawn from multivariate normal distributions with different means. Two background distributions are sampled from, which is meant to mimic the situation in particle physics where one has a 'nominal' prediction for a nuisance parameter and then an alternate value (e.g. from varying up/down by one standard deviation), which then modifies the background pdf. Here, we take that effect to be a shift of the mean of the distribution. The value for the background histogram is then the mean of the resulting counts of the two modes, and the uncertainty can be quantified through the count standard deviation.\n",
    "- `kde_counts_from_nn_three_blobs(predict, bins)` functions exactly as above, but uses a different method involving kernel density estimation to get the yields from the parameters of the observable, and needs the binning pre-specified as argument.\n",
    "- `nn_hepdata_like(hmaker)` uses the resulting functions from either of the above (or your own!) methods to construct histograms, then feeds them into the `neos.models.hepdata_like` function that constructs a pyhf-like model. This can then be used to call things like `logpdf` and `expected_data` downstream when CLs values are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import jax\n",
    "import jax.scipy as jsc\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from neos import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hists_from_nn_three_blobs(predict, NMC = 500, sig_mean = [-1, 1], b1_mean=[2, 2], b2_mean=[-1, -1], LUMI=10, sig_scale = 2, bkg_scale = 10):\n",
    "    '''\n",
    "    Uses the nn decision function `predict` to form histograms from signal and background \n",
    "    data, all drawn from multivariate normal distributions with different means. Two \n",
    "    background distributions are sampled from, which is meant to mimic the situation in \n",
    "    particle physics where one has a 'nominal' prediction for a nuisance parameter and then \n",
    "    an alternate value (e.g. from varying up/down by one standard deviation), which then \n",
    "    modifies the background pdf. Here, we take that effect to be a shift of the mean of the \n",
    "    distribution. The value for the background histogram is then the mean of the resulting \n",
    "    counts of the two modes, and the uncertainty can be quantified through the count \n",
    "    standard deviation.\n",
    "    \n",
    "    Args:\n",
    "            predict: Decision function for a parameterized observable. Assumed softmax here.\n",
    "\n",
    "    Returns:\n",
    "            hist_maker: A callable function that takes the parameters of the observable, \n",
    "            then constructs signal, background, and background uncertainty yields.\n",
    "    '''\n",
    "    def get_hists(network, s, bs):\n",
    "        NMC = len(s)\n",
    "        s_hist = predict(network, s).sum(axis=0) * sig_scale / NMC * LUMI\n",
    "        \n",
    "        b_hists = tuple(\n",
    "            (predict(network, b).sum(axis=0) * bkg_scale / NMC * LUMI) for b in bs\n",
    "        )\n",
    "        \n",
    "        b_mean = jax.numpy.mean(jax.numpy.asarray(b_hists), axis=0)\n",
    "        b_unc = jax.numpy.std(jax.numpy.asarray(b_hists), axis=0)\n",
    "        results = s_hist, b_mean, b_unc\n",
    "        return results\n",
    "\n",
    "\n",
    "    def hist_maker():\n",
    "        bkg1 = np.random.multivariate_normal(b1_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "        bkg2 = np.random.multivariate_normal(b2_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "        sig = np.random.multivariate_normal(sig_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "\n",
    "        def make(network):\n",
    "            return get_hists(network, sig, (bkg1,bkg2))\n",
    "\n",
    "        make.bkg1 = bkg1\n",
    "        make.bkg2 = bkg2\n",
    "        make.sig = sig\n",
    "        return make\n",
    "    \n",
    "    return hist_maker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# kde experiment\n",
    "\n",
    "def kde_counts_from_nn_three_blobs(predict, bins, bandwidth=.3, NMC = 500, sig_mean = [-1, 1], b1_mean=[2, 2], b2_mean=[-1, -1], LUMI=10, sig_scale = 2, bkg_scale = 10):\n",
    "    '''\n",
    "    Exactly the same as `hists_from_nn_three_blobs`, but takes in a regression network, and\n",
    "    forms a kernel density estimate (kde) for the output. The yields are then calculated as \n",
    "    the integral of the kde's cumulative density function between the bin edges, which should\n",
    "    be specified as an argument to the function.\n",
    "    \n",
    "    Args:\n",
    "            predict: Decision function for a parameterized observable. When evaluated, the \n",
    "            output should be one number per event, i.e. a regression network or similar.\n",
    "\n",
    "    Returns:\n",
    "            hist_maker: A callable function that takes the parameters of the observable, \n",
    "            then constructs signal, background, and background uncertainty yields.\n",
    "    '''\n",
    "    # grab bin edges\n",
    "    edge_lo   = bins[:-1]\n",
    "    edge_hi   = bins[1:]\n",
    "    \n",
    "    # get counts from gaussian cdfs centered on each event, evaluated binwise\n",
    "    def to_hist(events):\n",
    "        cdf_up = jsc.stats.norm.cdf(edge_hi.reshape(-1,1),loc = events, scale = bandwidth)\n",
    "        cdf_dn = jsc.stats.norm.cdf(edge_lo.reshape(-1,1),loc = events, scale = bandwidth)\n",
    "        summed = (cdf_up-cdf_dn).sum(axis=1)\n",
    "        return summed\n",
    "    \n",
    "    def get_hists(network, s, b1, b2):\n",
    "        NMC = len(s)\n",
    "        nn_s, nn_b1, nn_b2 = (\n",
    "            predict(network, s).ravel(),\n",
    "            predict(network, b1).ravel(),\n",
    "            predict(network, b2).ravel(),\n",
    "        )\n",
    "             \n",
    "        kde_counts = jax.numpy.asarray([\n",
    "            to_hist(nn_s)* sig_scale / NMC * LUMI,\n",
    "            to_hist(nn_b1)* bkg_scale / NMC * LUMI,\n",
    "            to_hist(nn_b2)* bkg_scale / NMC * LUMI,\n",
    "        ])\n",
    "        \n",
    "        b_mean = jax.numpy.mean(kde_counts[1:], axis=0)\n",
    "        b_unc = jax.numpy.std(kde_counts[1:], axis=0)\n",
    "        results = kde_counts[0], b_mean,b_unc\n",
    "        return results\n",
    "\n",
    "\n",
    "    def hist_maker():\n",
    "        bkg1 = np.random.multivariate_normal(b1_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "        bkg2 = np.random.multivariate_normal(b2_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "        sig = np.random.multivariate_normal(sig_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "\n",
    "        def make(network):\n",
    "            return get_hists(network, sig, bkg1, bkg2)\n",
    "\n",
    "        make.bkg1 = bkg1\n",
    "        make.bkg2 = bkg2\n",
    "        make.sig = sig\n",
    "        return make\n",
    "    \n",
    "    return hist_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pyhf\n",
    "pyhf.set_backend(pyhf.tensor.jax_backend())\n",
    "\n",
    "def nn_hepdata_like(histogram_maker):\n",
    "    '''\n",
    "    Returns a function that constructs a typical 'hepdata-like' statistical model\n",
    "    with signal, background, and background uncertainty yields when evaluated at\n",
    "    the parameters of the observable.\n",
    "    \n",
    "    Args:\n",
    "            histogram_maker: A function that, when called, returns a secondary function\n",
    "            that takes the observable's parameters as argument, and returns yields.\n",
    "         \n",
    "    Returns:\n",
    "            nn_model_maker: A function that returns a Model object (either from \n",
    "            `neos.models` or from `pyhf`) when evaluated at the observable's parameters.\n",
    "    '''\n",
    "    hm = histogram_maker()\n",
    "\n",
    "    def nn_model_maker(network):\n",
    "        s, b, db = hm(network)\n",
    "        print(type(s))\n",
    "        pprint(f's={s}, b={b}, db={db}')\n",
    "        m = pyhf.simplemodels.hepdata_like(s, b, db)\n",
    "        nompars = m.config.suggested_init()\n",
    "        bonlypars = jax.numpy.asarray([x for x in nompars])\n",
    "        bonlypars = jax.ops.index_update(bonlypars, m.config.poi_index, 0.0)\n",
    "        return m, bonlypars\n",
    "\n",
    "    nn_model_maker.hm = hm\n",
    "    return nn_model_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
