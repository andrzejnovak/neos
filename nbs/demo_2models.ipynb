{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing two models at once "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might be interested in optimizing for two \"compteting\" models at the same time. Consider having 3 separate samples A, B, C and we'd be interesting in extracting the significance for two out of the three at the same time. Two models would be fitted, e.g one where A is signal and B & C are backgrounds and one where B is signal and A & C are backgrounds. This example shows how to optimize for both of them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import neos.makers as makers\n",
    "import neos.cls as cls\n",
    "import numpy as np\n",
    "import jax.experimental.stax as stax\n",
    "import jax.experimental.optimizers as optimizers\n",
    "import jax.random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialse tools from `neos`:\n",
    "\n",
    "The way we initialise in `neos` is to define functions that make a statistical model from histograms, which in turn are themselves made from a predictive model, such as a neural network. Here's some detail on the unctions used below:\n",
    "\n",
    "- `hists_from_nn_three_blobs(predict)` uses the nn decision function `predict` defined in the cell above to form histograms from signal and background data, all drawn from multivariate normal distributions with different means. Two background distributions are sampled from, which is meant to mimic the situation in particle physics where one has a 'nominal' prediction for a nuisance parameter and then an alternate value (e.g. from varying up/down by one standard deviation), which then modifies the background pdf. Here, we take that effect to be a shift of the mean of the distribution. The value for the background histogram is then the mean of the resulting counts of the two modes, and the uncertainty can be quantified through the count standard deviation.\n",
    "- `nn_hepdata_like(hmaker)` uses `hmaker` to construct histograms, then feeds them into the `neos.models.hepdata_like` function that constructs a pyhf-like model. This can then be used to call things like `logpdf` and `expected_data` downstream.\n",
    "- `cls_maker` takes a model-making function as it's primary argument, which is fed into functions from `neos.fit` that minimise the `logpdf` of the model in both a constrained (fixed parameter of interest) and a global way. Moreover, these fits are wrapped in a function that allows us to calculate gradients through the fits using *fixed-point differentiation*. This allows for the calculation of both the profile likelihood and its gradient, and then the same for $\\mathsf{cl_s}$ :)\n",
    "\n",
    "All three of these methods return functions. in particular, `cls_maker` returns a function that differentiably calculates $\\mathsf{cl_s}$ values, which is our desired objective to minimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hists_from_nn_three_samples(\n",
    "        predict,\n",
    "        NMC=500,\n",
    "        s1_mean=[-2, 2],\n",
    "        s2_mean=[2, 2],\n",
    "        s3_mean=[0, -2],\n",
    "        LUMI=10,\n",
    "        sig_scale=1,\n",
    "        bkg_scale=1,\n",
    "        group=1,\n",
    "        real_z=False,\n",
    "):\n",
    "    '''\n",
    "    Same as hists_from_nn_three_blobs, but parametrize grouping of signal and background for\n",
    "    three separatate samples\n",
    "    Args:\n",
    "            predict: Decision function for a parameterized observable. Assumed softmax here.\n",
    "\n",
    "    Returns:\n",
    "            hist_maker: A callable function that takes the parameters of the observable,\n",
    "            then constructs signal, background, and background uncertainty yields.\n",
    "    '''\n",
    "    def get_hists(network, s, bs):\n",
    "        NMC = len(s)\n",
    "        s_hist = predict(network, s).sum(axis=0) * sig_scale / NMC * LUMI\n",
    "\n",
    "        b_hists = tuple([\n",
    "            predict(network, bs[0]).sum(axis=0) * sig_scale / NMC * LUMI,\n",
    "            predict(network, bs[1]).sum(axis=0) * bkg_scale / NMC * LUMI\n",
    "        ])\n",
    "\n",
    "        b_tot = jax.numpy.sum(jax.numpy.asarray(b_hists), axis=0)\n",
    "        b_unc = jax.numpy.sqrt(b_tot)\n",
    "        # append raw hists for signal and bkg as well\n",
    "        results = s_hist, b_tot, b_unc, s_hist, b_hists\n",
    "        return results\n",
    "\n",
    "\n",
    "    def hist_maker():\n",
    "        sig1 = np.random.multivariate_normal(s1_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "        sig2 = np.random.multivariate_normal(s2_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "        bkg = np.random.multivariate_normal(s3_mean, [[1, 0], [0, 1]], size=(NMC,))\n",
    "\n",
    "        def make(network):\n",
    "            if group == 1:\n",
    "                return get_hists(network, sig1, (sig2, bkg))\n",
    "            elif group == 2:\n",
    "                return get_hists(network, sig2, (sig1, bkg))\n",
    "            elif group == 3:\n",
    "                return get_hists(network, bkg, (sig1, sig2))\n",
    "            else:\n",
    "                raise UserWarning\n",
    "\n",
    "        make.bkg = bkg\n",
    "        make.sig2 = sig2\n",
    "        make.sig1 = sig1\n",
    "        return make\n",
    "    \n",
    "    return hist_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "pyhf.set_backend(pyhf.tensor.jax_backend())\n",
    "\n",
    "from neos import models\n",
    "\n",
    "def nn_hepdata_like_w_hists(histogram_maker):\n",
    "    '''\n",
    "    Analogous function to `makers.nn_hepdata_like`, but modified to pass through\n",
    "    the additional info added in hists_from_nn_three_samples.\n",
    "    '''\n",
    "    hm = histogram_maker()\n",
    "    \n",
    "    def nn_model_maker(network):\n",
    "        s, b, db, _, _ = hm(network) # Changed here\n",
    "        m = models.hepdata_like(s, b, db) # neos model\n",
    "        nompars = m.config.suggested_init()\n",
    "        bonlypars = jax.numpy.asarray([x for x in nompars])\n",
    "        bonlypars = jax.ops.index_update(bonlypars, m.config.poi_index, 0.0)\n",
    "        return m, bonlypars\n",
    "\n",
    "    nn_model_maker.hm = hm\n",
    "    return nn_model_maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network using `jax.experimental.stax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUT = 3\n",
    "init_random_params, predict = stax.serial(\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(NOUT),\n",
    "    stax.Softmax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hitsogram and model maker functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmaker = hists_from_nn_three_samples(predict, group=1)\n",
    "nnm = nn_hepdata_like_w_hists(hmaker)\n",
    "\n",
    "hmaker2 = hists_from_nn_three_samples(predict, group=2)\n",
    "nnm2 = nn_hepdata_like_w_hists(hmaker2)\n",
    "\n",
    "loss1 = cls.cls_maker(nnm, solver_kwargs=dict(pdf_transform=True))\n",
    "loss2 = cls.cls_maker(nnm2, solver_kwargs=dict(pdf_transform=True))\n",
    "\n",
    "loss = lambda params, test_mu: (loss1(params, test_mu) + loss2(params, test_mu))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly initialise nn weights and check that we can get the gradient of the loss wrt nn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.11794581, dtype=float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, network = init_random_params(jax.random.PRNGKey(2), (-1, 2))\n",
    "\n",
    "loss(network,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([3.18964125, 3.35441495, 3.4559438 ], dtype=float64),\n",
       " DeviceArray([6.61912132, 7.00347019, 6.3774085 ], dtype=float64),\n",
       " DeviceArray([2.57276531, 2.64640703, 2.52535314], dtype=float64),\n",
       " DeviceArray([3.18964125, 3.35441495, 3.4559438 ], dtype=float64),\n",
       " (DeviceArray([3.14678753, 3.7319485 , 3.12126397], dtype=float64),\n",
       "  DeviceArray([3.47233379, 3.27152169, 3.25614452], dtype=float64)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnm.hm(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d, e = nnm.hm(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jit_loss = jax.jit(loss)\n",
    "\n",
    "opt_init, opt_update, opt_params = optimizers.adam(.5e-3)\n",
    "\n",
    "def train_network(N, cont=False, network=None):\n",
    "    if not cont:\n",
    "        _, network = init_random_params(jax.random.PRNGKey(4), (-1, 2))\n",
    "    if network is not None:\n",
    "        network = network\n",
    "    losses = []\n",
    "    cls_vals = []\n",
    "    state = opt_init(network)\n",
    "    \n",
    "    # parameter update function\n",
    "    #@jax.jit\n",
    "    def update_and_value(i, opt_state, mu, loss_choice):\n",
    "        net = opt_params(opt_state)\n",
    "        value, grad = jax.value_and_grad(loss_choice)(net, mu)\n",
    "        return opt_update(i, grad, state), value, net\n",
    "    \n",
    "    for i in range(N):\n",
    "        start_time = time.time()\n",
    "        loss_choice = loss\n",
    "        state, value, network = update_and_value(i,state,1.0, loss_choice)\n",
    "        epoch_time = time.time() - start_time\n",
    "        losses.append(value)\n",
    "        metrics = {\"loss\": losses}\n",
    "        yield network, metrics, epoch_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting helper function for awesome animations :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose colormap\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "def to_transp(cmap):\n",
    "    #cmap = pl.cm.Reds_r\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    #my_cmap[:,-1] = np.geomspace(0.001, 1, cmap.N)\n",
    "    my_cmap[:,-1] = np.linspace(0, 0.7, cmap.N)\n",
    "    #my_cmap[:,-1] = np.ones(cmap.N)\n",
    "    return ListedColormap(my_cmap)\n",
    "\n",
    "def plot(axarr, network, metrics, hm, hm2, maxN, ith):\n",
    "    xlim = (-5, 5)\n",
    "    ylim = (-5, 5)\n",
    "    g = np.mgrid[xlim[0]:xlim[1]:101j, ylim[0]:ylim[1]:101j]\n",
    "    levels = np.linspace(0, 1, 20)\n",
    "        \n",
    "    ax = axarr[0]\n",
    "    ax.contourf(\n",
    "        g[0],\n",
    "        g[1],\n",
    "        predict(network, np.moveaxis(g, 0, -1)).reshape(101, 101, NOUT)[:, :, 0],\n",
    "        levels=levels,\n",
    "        cmap = to_transp(pl.cm.Reds),\n",
    "    )\n",
    "    ax.contourf(\n",
    "        g[0],\n",
    "        g[1],\n",
    "        predict(network, np.moveaxis(g, 0, -1)).reshape(101, 101, NOUT)[:, :, 1],\n",
    "        levels=levels,\n",
    "        cmap = to_transp(pl.cm.Greens),\n",
    "    )\n",
    "    if NOUT > 2:\n",
    "        ax.contourf(\n",
    "            g[0],\n",
    "            g[1],\n",
    "            predict(network, np.moveaxis(g, 0, -1)).reshape(101, 101, 3)[:, :, 2],\n",
    "            levels=levels,\n",
    "            cmap = to_transp(pl.cm.Blues),\n",
    "        )\n",
    "    \n",
    "    #print(list(map(len, [hm.sig1[:, 0], hm.sig2[:, 0], hm.bkg[:, 0]])))\n",
    "    ax.scatter(hm.sig1[:, 0], hm.sig1[:, 1], alpha=0.25, c=\"C9\", label=\"sig1\")\n",
    "    ax.scatter(hm.sig2[:, 0], hm.sig2[:, 1], alpha=0.17, c=\"C8\", label=\"bkg2\")\n",
    "    ax.scatter(hm.bkg[:, 0], hm.bkg[:, 1], alpha=0.17, c=\"C1\", label=\"bkg2\")\n",
    "    ax.set_xlim(*xlim)\n",
    "    ax.set_ylim(*ylim)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    ax = axarr[1]\n",
    "    ax.axhline(0.05, c=\"slategray\", linestyle=\"--\")\n",
    "    ax.plot(metrics[\"loss\"][:ith], c=\"steelblue\", linewidth=2.0)\n",
    "\n",
    "    ax.set_ylim(0, metrics[\"loss\"][0])\n",
    "    ax.set_xlim(0, maxN)\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(r\"$cl_s$\")\n",
    "\n",
    "    ax = axarr[2]\n",
    "    s, b, db, sig, bs = hm(network)\n",
    "    ytop = np.max(np.sum([s, b], axis=0))*1.3\n",
    "    ax.bar(range(NOUT), sig, bottom=bs[0]+bs[1], color=\"C9\", label=\"Sample 1\")\n",
    "    ax.bar(range(NOUT), bs[0], bottom=bs[1], color=\"C8\", label=\"Sample 2\")\n",
    "    ax.bar(range(NOUT), bs[1], color=\"C1\", label=\"Sample 3\")\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_xlabel(\"nn output\")\n",
    "    ax.set_title(\"Raw histograms\")\n",
    "    ax.set_ylim(0, ytop)\n",
    "    if ith == 0:\n",
    "        ax.legend()\n",
    "        \n",
    "    ax = axarr[3]\n",
    "    s, b, db, sig, bs = hm(network)\n",
    "    ax.bar(range(NOUT), s, bottom=b, color=\"#722620\", label=\"sig\", alpha=0.9)\n",
    "    ax.bar(range(NOUT), b, color=\"#F2BC94\", label=\"bkg\")\n",
    "    ax.bar(range(NOUT), db, bottom=b - db / 2.0, alpha=0.3, color=\"black\", label=\"bkg error\", hatch='////')\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_xlabel(\"nn output\")\n",
    "    ax.set_title(\"Model 1: sig1 vs (sig2 + bkg)\")\n",
    "    ax.set_ylim(0, ytop)\n",
    "    if ith == 0:\n",
    "        ax.legend()\n",
    "        \n",
    "    ax = axarr[4]\n",
    "    s, b, db, sig, bs = hm2(network)\n",
    "    ax.bar(range(NOUT), s, bottom=b, color=\"#722620\", label=\"sig\")\n",
    "    ax.bar(range(NOUT), b, color=\"#F2BC94\", label=\"bkg\")\n",
    "    ax.bar(range(NOUT), db, bottom=b - db / 2.0, alpha=0.3, color=\"black\", label=\"bkg error\", hatch='////')\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_xlabel(\"nn output\")\n",
    "    ax.set_title(\"Model 2: sig2 vs (sig1 + bkg)\")\n",
    "    ax.set_ylim(0, ytop)\n",
    "    if ith == 0:\n",
    "        ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run it!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: CLs = 0.11744840634690035, took 13.996879816055298s\n",
      "epoch 1: CLs = 0.08108947633857888, took 6.273714542388916s\n",
      "epoch 2: CLs = 0.041910327657380564, took 6.2989325523376465s\n",
      "epoch 3: CLs = 0.017528712819379733, took 6.52151083946228s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.labelsize\": 13,\n",
    "        \"axes.linewidth\": 1.2,\n",
    "        \"xtick.labelsize\": 13,\n",
    "        \"ytick.labelsize\": 13,\n",
    "        \"figure.figsize\": [7.2, 3.0],\n",
    "        \"font.size\": 13,\n",
    "        \"xtick.major.size\": 4,\n",
    "        \"ytick.major.size\": 4,\n",
    "        \"legend.fontsize\": 11,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "fig, axarr = plt.subplots(2, 3)\n",
    "axarr = axarr.flatten()\n",
    "fig.set_size_inches(15, 10)\n",
    "camera = Camera(fig)\n",
    "\n",
    "maxN = 10 # make me bigger for better results!\n",
    "\n",
    "animate = True # animations fail tests\n",
    "\n",
    "# Training\n",
    "for i, (network, metrics, epoch_time) in enumerate(train_network(maxN)):\n",
    "    print(f\"epoch {i}:\", f'CLs = {metrics[\"loss\"][-1]}, took {epoch_time}s')\n",
    "    if animate:\n",
    "        if i % 6 == 0:\n",
    "            plot(axarr, network, metrics, nnm.hm, nnm2.hm, maxN=maxN, ith=i)\n",
    "            plt.tight_layout()\n",
    "            camera.snap()\n",
    "            camera.animate().save(\"animation.gif\", writer=\"imagemagick\", fps=6)\n",
    "            #HTML(camera.animate().to_html5_video())\n",
    "if animate:\n",
    "    camera.animate().save(\"animation.gif\", writer=\"imagemagick\", fps=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
